{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10239648,"sourceType":"datasetVersion","datasetId":6332174},{"sourceId":10239771,"sourceType":"datasetVersion","datasetId":6332270},{"sourceId":10254000,"sourceType":"datasetVersion","datasetId":6342818},{"sourceId":10265362,"sourceType":"datasetVersion","datasetId":6350743},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:31:55.791208Z","iopub.execute_input":"2024-12-22T19:31:55.791501Z","iopub.status.idle":"2024-12-22T19:31:56.091792Z","shell.execute_reply.started":"2024-12-22T19:31:55.791478Z","shell.execute_reply":"2024-12-22T19:31:56.090982Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model.safetensors.index.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00001-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00003-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00002-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00007-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/README.md\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00008-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00005-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00006-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/special_tokens_map.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/.gitattributes\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.model\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00004-of-00008.safetensors\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/generation_config.json\n/kaggle/input/gemma-2/transformers/gemma-2-9b/2/transformers/transformers-4.42.0.dev0-py3-none-any.whl\n/kaggle/input/santa1/sample_submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-9b\")\n\nprint(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:36:45.489176Z","iopub.execute_input":"2024-12-22T18:36:45.489625Z","iopub.status.idle":"2024-12-22T18:36:47.244094Z","shell.execute_reply.started":"2024-12-22T18:36:45.489595Z","shell.execute_reply":"2024-12-22T18:36:47.243297Z"}},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/gemma-2/transformers/gemma-2-9b/2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Perplexity Calculator Using Pretrained Language Models :","metadata":{}},{"cell_type":"markdown","source":"This Python class, PerplexityCalculator, leverages the Transformers library by Hugging Face to calculate the perplexity of given text(s) using a pretrained causal language model (e.g., GPT). The implementation supports both single-text and batch processing, and it allows loading models in standard precision or 8-bit quantized mode to optimize memory usage.","metadata":{}},{"cell_type":"code","source":"import random\nimport gc\nimport torch\nimport pandas as pd\nfrom collections import Counter\nfrom typing import List, Union\nimport numpy as np\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass PerplexityCalculator:\n    def __init__(self, model_path: str, load_in_8bit: bool = False, device_map: str = 'auto'):\n        \"\"\"\n        Initialize the Perplexity Calculator with a pretrained model and tokenizer.\n\n        Args:\n            model_path (str): Path to the pretrained model or model name from Hugging Face's model hub.\n            load_in_8bit (bool): Whether to load the model in 8-bit precision for memory efficiency.\n            device_map (str): Device configuration for loading the model (e.g., 'auto' for automatic allocation).\n        \"\"\"\n        # Load the tokenizer with trust_remote_code=True for custom models\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            trust_remote_code=True\n        )\n        \n        # Load the model with the appropriate precision settings\n        if load_in_8bit:\n            # Use 8-bit quantization for memory-efficient model loading\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n                trust_remote_code=True\n            )\n        else:\n            # Load the model in full precision (float16 for GPUs, float32 otherwise)\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=device_map,\n                trust_remote_code=True\n            )\n\n        # Initialize the loss function for calculating perplexity\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        \n        # Set the model to evaluation mode\n        self.model.eval()\n\n    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 8) -> Union[float, List[float]]:\n        \"\"\"\n        Compute the perplexity for a single text or a batch of texts.\n\n        Args:\n            input_texts (Union[str, List[str]]): A single string or a list of strings for perplexity computation.\n            batch_size (int): Number of texts to process in a single batch.\n\n        Returns:\n            Union[float, List[float]]: The perplexity for the input text(s).\n        \"\"\"\n        # Check if the input is a single text or a list of texts\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n        \n        # List to store the loss values for each text\n        loss_list = []\n\n        # Process the input texts in batches\n        for i in range(0, len(input_texts), batch_size):\n            batch_texts = input_texts[i:i + batch_size]\n            with torch.no_grad():\n                # Tokenize the batch of texts\n                model_inputs = self.tokenizer(\n                    batch_texts,\n                    return_tensors='pt',\n                    padding=True,\n                    truncation=True,\n                    max_length=512,\n                    add_special_tokens=True,\n                ).to(self.model.device)\n\n                # Forward pass through the model\n                output = self.model(**model_inputs)\n                logits = output.logits\n\n                # Shift the logits and labels to align predictions with the next token\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n\n                # Calculate the loss for the batch\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Normalize the loss per token and store the results\n                batch_losses = loss.view(len(batch_texts), -1).sum(dim=1) / model_inputs['attention_mask'].sum(dim=1)\n                loss_list.extend(batch_losses.cpu().tolist())\n\n        # Convert loss to perplexity (PPL = exp(loss))\n        ppl = [np.exp(i) for i in loss_list]\n        \n        # Return a single value if input was a single text, else return a list\n        return ppl[0] if single_input else ppl\n\n    def clear_memory(self) -> None:\n        \"\"\"\n        Clear memory by deleting the model and tokenizer to free up resources.\n        \"\"\"\n        del self.model\n        del self.tokenizer\n        gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:33:56.668211Z","iopub.execute_input":"2024-12-22T19:33:56.668522Z","iopub.status.idle":"2024-12-22T19:34:00.568887Z","shell.execute_reply.started":"2024-12-22T19:33:56.668490Z","shell.execute_reply":"2024-12-22T19:34:00.568038Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# GeneticAlgorithm class :","metadata":{}},{"cell_type":"code","source":"import random\nimport gc\n\n# Define the GeneticAlgorithm class\nclass GeneticAlgorithm:\n    def __init__(self, random_state, population_size, generations, mutation_rate, elite_rate):\n        \"\"\"\n        Initialize the GeneticAlgorithm class.\n\n        Parameters:\n        - random_state (int): Seed for random number generation to ensure reproducibility.\n        - population_size (int): Number of individuals in the population.\n        - generations (int): Number of generations to run the algorithm.\n        - mutation_rate (float): Probability of mutation for each individual.\n        - elite_rate (float): Fraction of the population to retain as elite individuals.\n        \"\"\"\n        random.seed(random_state)\n        self.population_size = population_size\n        self.generations = generations\n        self.mutation_rate = mutation_rate\n        self.elite_rate = elite_rate\n\n    def order_crossover(self, parent1, parent2):\n        \"\"\"\n        Perform order crossover between two parents to produce an offspring.\n\n        Parameters:\n        - parent1 (list): First parent individual.\n        - parent2 (list): Second parent individual.\n\n        Returns:\n        - child (list): New individual created from parents.\n        \"\"\"\n        # Select a random slice of the parent1\n        start, end = sorted(random.sample(range(len(parent1)), 2))\n        child = [None] * len(parent1)\n        child[start:end+1] = parent1[start:end+1]\n\n        # Fill remaining positions using elements from parent2\n        remaining = parent2.copy()\n        for used_word in child:\n            if used_word is not None:\n                remaining.remove(used_word)\n        j = 0\n        for i in range(len(child)):\n            if child[i] is None:\n                child[i] = remaining[j]\n                j += 1\n        return child\n\n    def mutate(self, individual):\n        \"\"\"\n        Apply mutation to an individual by swapping two positions with a given probability.\n\n        Parameters:\n        - individual (list): The individual to mutate.\n\n        Returns:\n        - individual (list): Mutated individual.\n        \"\"\"\n        if random.random() < self.mutation_rate:\n            i, j = random.sample(range(len(individual)), 2)\n            individual[i], individual[j] = individual[j], individual[i]\n        return individual\n\n    def gradient_descent(self, sequence, scorer, learning_rate=0.01, max_iterations=20):\n        \"\"\"\n        Apply a simple gradient descent-inspired optimization to refine an individual.\n\n        Parameters:\n        - sequence (list): Initial sequence to optimize.\n        - scorer (object): Scorer object to evaluate perplexity.\n        - learning_rate (float): Not used directly, kept for extension purposes.\n        - max_iterations (int): Maximum number of iterations for optimization.\n\n        Returns:\n        - best_sequence (list): Optimized sequence with the lowest perplexity.\n        \"\"\"\n        current_sequence = sequence[:]\n        best_sequence = sequence[:]\n        best_perplexity = scorer.get_perplexity(' '.join(best_sequence))\n\n        # Iteratively improve the sequence by swapping elements\n        for _ in range(max_iterations):\n            neighbor = current_sequence[:]\n            i, j = random.sample(range(len(neighbor)), 2)\n            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n            neighbor_perplexity = scorer.get_perplexity(' '.join(neighbor))\n            delta = neighbor_perplexity - best_perplexity\n            if delta < 0:  # If the perplexity improves\n                current_sequence = neighbor[:]\n                if neighbor_perplexity < best_perplexity:\n                    best_sequence = neighbor[:]\n                    best_perplexity = neighbor_perplexity\n\n        return best_sequence\n\n    def solve(self, text, scorer):\n        \"\"\"\n        Solve the optimization problem using the genetic algorithm.\n\n        Parameters:\n        - text (str): Input text to optimize.\n        - scorer (object): Scorer object to calculate perplexity.\n\n        Returns:\n        - best_sequence (str): Optimized sequence as a single string.\n        - best_energy (float): Perplexity of the best sequence.\n        - log_energies (list): List of best perplexities per generation.\n        \"\"\"\n        # Split the text into words and create the initial population\n        words = text.split()\n        word_N = len(words)\n        population = [random.sample(words, word_N) for _ in range(self.population_size)]\n        log_energies = []\n\n        # Main loop for generations\n        for generation in range(self.generations):\n            # Calculate fitness (perplexity) for all individuals\n            fitness_scores = [scorer.get_perplexity(' '.join(ind)) for ind in population]\n            min_perplexity = min(fitness_scores)\n            log_energies.append(min_perplexity)\n            print(f\"Generation {generation + 1}/{self.generations} - Best Perplexity: {min_perplexity}\")\n\n            # Select elite individuals based on perplexity\n            elite_indices = sorted(range(len(fitness_scores)),\n                                   key=lambda k: fitness_scores[k])[:int(self.population_size * self.elite_rate)]\n            new_population = [population[i] for i in elite_indices]\n\n            # Create new individuals via crossover until the population is restored\n            while len(new_population) < self.population_size:\n                parent1, parent2 = random.sample(population, 2)\n                child = self.order_crossover(parent1, parent2)\n                new_population.append(child)\n\n            # Mutate new population\n            for i in range(len(new_population)):\n                new_population[i] = self.mutate(new_population[i])\n\n            # Apply gradient descent to elite individuals for further improvement\n            for i in range(int(len(new_population) * self.elite_rate)):\n                new_population[i] = self.gradient_descent(new_population[i], scorer)\n\n            population = new_population\n\n        # Calculate final fitness scores and select the best individual\n        fitness_scores = [scorer.get_perplexity(' '.join(ind)) for ind in population]\n        best_individual = population[np.argmin(fitness_scores)]\n        best_energy = min(fitness_scores)\n        print(\"\\nGenetic Algorithm Optimization Complete\")\n        print(f\"Best Perplexity: {best_energy}\")\n\n        return ' '.join(best_individual), best_energy, log_energies\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:33:49.518786Z","iopub.execute_input":"2024-12-22T19:33:49.519238Z","iopub.status.idle":"2024-12-22T19:33:49.531346Z","shell.execute_reply.started":"2024-12-22T19:33:49.519212Z","shell.execute_reply":"2024-12-22T19:33:49.530568Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Optimizing Text Sequences with Genetic Algorithm for Perplexity Minimization","metadata":{}},{"cell_type":"markdown","source":"We have split the data into two parts to optimize processing time and resource usage. The first batch contains sequences 1 to 5, while the second batch contains sequence 6. This division allows us to handle smaller chunks of data at a time, minimizing the computational load and ensuring more efficient use of resources during processing.","metadata":{}},{"cell_type":"markdown","source":"This script processes and optimizes text sequences using a Genetic Algorithm (GA) for the first batch of a given dataset.","metadata":{}},{"cell_type":"code","source":"# Function to optimize sequences using Genetic Algorithm (GA) for the first batch (10 generations)\ndef optimize_sequences_with_ga(batch_size=5):\n    \"\"\"\n    Optimize text sequences using a Genetic Algorithm (GA) for the first batch of data.\n\n    Parameters:\n    - batch_size (int): Number of rows to process per batch.\n\n    Returns:\n    - submission (DataFrame): DataFrame containing the optimized text sequences.\n    \"\"\"\n    # Load the sample submission file\n    sample_submission = pd.read_csv(\"/kaggle/input/santa1/sample_submission.csv\")\n    results = []\n\n    # Initialize the Genetic Algorithm with specific parameters\n    ga = GeneticAlgorithm(\n        random_state=42,      # Seed for reproducibility\n        population_size=10,   # Number of individuals in the population\n        generations=20,       # Number of generations for optimization\n        mutation_rate=0.1,    # Mutation probability for individuals\n        elite_rate=0.2        # Fraction of population considered as elite\n    )\n\n    # Determine the number of batches based on batch size\n    num_batches = (len(sample_submission) + batch_size - 1) // batch_size\n    batch_idx = 0  # Process only the first batch\n\n    # Define the data range for the current batch\n    batch_start = batch_idx * batch_size\n    batch_end = min((batch_idx + 1) * batch_size, len(sample_submission))\n    batch_data = sample_submission.iloc[batch_start:batch_end]\n\n    print(f\"\\nProcessing batch {batch_idx + 1}/{num_batches}\")\n\n    try:\n        # Initialize the scorer (PerplexityCalculator) for text evaluation\n        scorer = PerplexityCalculator(\n            model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2',  # Path to the language model\n            load_in_8bit=False  # Model loading option\n        )\n\n        # Iterate over rows in the batch\n        for idx, row in batch_data.iterrows():\n            # Create a specific DataFrame for the current row\n            specific_solution = pd.DataFrame({'id': [row['id']], 'text': [row['text']]})\n            text_sequence = row['text'].split()\n\n            # Optimize the text sequence using the Genetic Algorithm\n            optimized_text, final_score, log_energies = ga.solve(row['text'], scorer)\n\n            # Log the result for the current sequence\n            print(f\"ID: {row['id']}, Final Perplexity: {final_score}\")\n            results.append({'id': row['id'], 'text': optimized_text})\n\n        # Save intermediate results to a temporary CSV file\n        temp_df = pd.DataFrame(results)\n        temp_df.to_csv(f\"submission_temp_batch_{batch_idx+1}.csv\", index=False)\n\n    except Exception as e:\n        # Handle exceptions and log any issues encountered during processing\n        print(f\"Error processing batch {batch_idx + 1}: {str(e)}\")\n        for idx, row in batch_data.iterrows():\n            # If an error occurs, retain the original text sequence\n            results.append({'id': row['id'], 'text': row['text']})\n            print(\"results >>>>>>>>>>>>>>>>\")\n            print({'id': row['id'], 'text': row['text']})\n            print(\"------------------------------------------------\")\n\n    # Final submission for the first batch\n    submission = pd.DataFrame(results)\n    submission.to_csv(\"batch1_submission.csv\", index=False)\n    return submission\n\nif __name__ == \"__main__\":\n    # Entry point of the script\n    print(\"Starting GA optimization...\")\n    final_submission = optimize_sequences_with_ga()\n    print(\"Optimization with GA completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:34:27.886502Z","iopub.execute_input":"2024-12-22T19:34:27.886968Z","iopub.status.idle":"2024-12-22T21:14:33.853532Z","shell.execute_reply.started":"2024-12-22T19:34:27.886923Z","shell.execute_reply":"2024-12-22T21:14:33.852658Z"}},"outputs":[{"name":"stdout","text":"Starting GA optimization...\n\nProcessing batch 1/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60a60f11dc404e95aad4038afd2c9812"}},"metadata":{}},{"name":"stdout","text":"Generation 1/20 - Best Perplexity: 850.8240325202685\nGeneration 2/20 - Best Perplexity: 621.4654194025813\nGeneration 3/20 - Best Perplexity: 610.160500583438\nGeneration 4/20 - Best Perplexity: 374.05994496126846\nGeneration 5/20 - Best Perplexity: 374.05994496126846\nGeneration 6/20 - Best Perplexity: 350.0905961076616\nGeneration 7/20 - Best Perplexity: 336.633887513654\nGeneration 8/20 - Best Perplexity: 336.633887513654\nGeneration 9/20 - Best Perplexity: 317.99778895332923\nGeneration 10/20 - Best Perplexity: 317.99778895332923\nGeneration 11/20 - Best Perplexity: 317.99778895332923\nGeneration 12/20 - Best Perplexity: 317.99778895332923\nGeneration 13/20 - Best Perplexity: 317.99778895332923\nGeneration 14/20 - Best Perplexity: 317.99778895332923\nGeneration 15/20 - Best Perplexity: 317.99778895332923\nGeneration 16/20 - Best Perplexity: 317.99778895332923\nGeneration 17/20 - Best Perplexity: 317.99778895332923\nGeneration 18/20 - Best Perplexity: 317.99778895332923\nGeneration 19/20 - Best Perplexity: 317.99778895332923\nGeneration 20/20 - Best Perplexity: 317.99778895332923\n\nGenetic Algorithm Optimization Complete\nBest Perplexity: 317.99778895332923\nID: 0, Final Perplexity: 317.99778895332923\nGeneration 1/20 - Best Perplexity: 2661.621005172972\nGeneration 2/20 - Best Perplexity: 2116.148403180039\nGeneration 3/20 - Best Perplexity: 1101.4482456601258\nGeneration 4/20 - Best Perplexity: 787.9337741951443\nGeneration 5/20 - Best Perplexity: 787.9337741951443\nGeneration 6/20 - Best Perplexity: 787.9337741951443\nGeneration 7/20 - Best Perplexity: 787.9337741951443\nGeneration 8/20 - Best Perplexity: 725.891068053924\nGeneration 9/20 - Best Perplexity: 752.1880180343552\nGeneration 10/20 - Best Perplexity: 752.1880180343552\nGeneration 11/20 - Best Perplexity: 569.6030465775146\nGeneration 12/20 - Best Perplexity: 569.6030465775146\nGeneration 13/20 - Best Perplexity: 563.9113404293024\nGeneration 14/20 - Best Perplexity: 563.9113404293024\nGeneration 15/20 - Best Perplexity: 560.1553570447047\nGeneration 16/20 - Best Perplexity: 560.1553570447047\nGeneration 17/20 - Best Perplexity: 560.1553570447047\nGeneration 18/20 - Best Perplexity: 560.1553570447047\nGeneration 19/20 - Best Perplexity: 560.1553570447047\nGeneration 20/20 - Best Perplexity: 560.1553570447047\n\nGenetic Algorithm Optimization Complete\nBest Perplexity: 563.9113404293024\nID: 1, Final Perplexity: 563.9113404293024\nGeneration 1/20 - Best Perplexity: 1177.8470978997168\nGeneration 2/20 - Best Perplexity: 743.9148323505923\nGeneration 3/20 - Best Perplexity: 665.1004029702837\nGeneration 4/20 - Best Perplexity: 657.571513566096\nGeneration 5/20 - Best Perplexity: 542.5128216498547\nGeneration 6/20 - Best Perplexity: 478.9894360971648\nGeneration 7/20 - Best Perplexity: 392.0234199570433\nGeneration 8/20 - Best Perplexity: 385.9850452914495\nGeneration 9/20 - Best Perplexity: 385.9850452914495\nGeneration 10/20 - Best Perplexity: 385.9850452914495\nGeneration 11/20 - Best Perplexity: 395.12918454981354\nGeneration 12/20 - Best Perplexity: 395.12918454981354\nGeneration 13/20 - Best Perplexity: 385.3775999821293\nGeneration 14/20 - Best Perplexity: 378.9111086460502\nGeneration 15/20 - Best Perplexity: 374.0788522012974\nGeneration 16/20 - Best Perplexity: 367.44732362836413\nGeneration 17/20 - Best Perplexity: 367.44732362836413\nGeneration 18/20 - Best Perplexity: 367.44732362836413\nGeneration 19/20 - Best Perplexity: 367.44732362836413\nGeneration 20/20 - Best Perplexity: 367.44732362836413\n\nGenetic Algorithm Optimization Complete\nBest Perplexity: 367.44732362836413\nID: 2, Final Perplexity: 367.44732362836413\nGeneration 1/20 - Best Perplexity: 1396.6020345581037\nGeneration 2/20 - Best Perplexity: 1079.1774716469092\nGeneration 3/20 - Best Perplexity: 929.2056505227479\nGeneration 4/20 - Best Perplexity: 874.4931377099937\nGeneration 5/20 - Best Perplexity: 805.777578225876\nGeneration 6/20 - Best Perplexity: 748.8926135405061\nGeneration 7/20 - Best Perplexity: 748.8926135405061\nGeneration 8/20 - Best Perplexity: 702.2867476505257\nGeneration 9/20 - Best Perplexity: 697.6101000687772\nGeneration 10/20 - Best Perplexity: 685.6968100236877\nGeneration 11/20 - Best Perplexity: 681.3547785398704\nGeneration 12/20 - Best Perplexity: 685.6968100236877\nGeneration 13/20 - Best Perplexity: 741.4288173444086\nGeneration 14/20 - Best Perplexity: 540.2962475336747\nGeneration 15/20 - Best Perplexity: 540.2962475336747\nGeneration 16/20 - Best Perplexity: 430.6810093953382\nGeneration 17/20 - Best Perplexity: 430.6810093953382\nGeneration 18/20 - Best Perplexity: 430.64774159748873\nGeneration 19/20 - Best Perplexity: 430.64774159748873\nGeneration 20/20 - Best Perplexity: 390.9444444246592\n\nGenetic Algorithm Optimization Complete\nBest Perplexity: 390.9444444246592\nID: 3, Final Perplexity: 390.9444444246592\nGeneration 1/20 - Best Perplexity: 1358.0371469788124\nGeneration 2/20 - Best Perplexity: 1055.4272247305064\nGeneration 3/20 - Best Perplexity: 921.3601144622896\nGeneration 4/20 - Best Perplexity: 809.2962839758362\nGeneration 5/20 - Best Perplexity: 798.6660251074331\nGeneration 6/20 - Best Perplexity: 731.6080242139705\nGeneration 7/20 - Best Perplexity: 728.7640948303405\nGeneration 8/20 - Best Perplexity: 678.9573802904102\nGeneration 9/20 - Best Perplexity: 660.9111689803203\nGeneration 10/20 - Best Perplexity: 657.3987677134307\nGeneration 11/20 - Best Perplexity: 634.4090522473495\nGeneration 12/20 - Best Perplexity: 619.2359300747122\nGeneration 13/20 - Best Perplexity: 619.2359300747122\nGeneration 16/20 - Best Perplexity: 558.8739298244708\nGeneration 17/20 - Best Perplexity: 558.8739298244708\nGeneration 18/20 - Best Perplexity: 524.6098174084065\nGeneration 19/20 - Best Perplexity: 516.2489879094545\nGeneration 20/20 - Best Perplexity: 497.4217829188407\n\nGenetic Algorithm Optimization Complete\nBest Perplexity: 495.3355844563759\nID: 4, Final Perplexity: 495.3355844563759\nOptimization with GA completed!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Genetic Algorithm Optimization for Text Sequences: Second Batch Processing","metadata":{}},{"cell_type":"code","source":"# Function to optimize sequences with GA for second batch\ndef optimize_sequences_with_ga(batch_size=5):\n    sample_submission = pd.read_csv(\"/kaggle/input/santa1/sample_submission.csv\")\n    results = []\n\n    ga = GeneticAlgorithm(\n        random_state=42,\n        population_size=10,\n        generations=20,\n        mutation_rate=0.1,\n        elite_rate=0.2\n    )\n\n    # Calculate the total number of batches\n    num_batches = (len(sample_submission) + batch_size - 1) // batch_size\n\n    # Set batch_idx to 1 for the second batch (index starts at 0)\n    batch_idx = 1  # Only process the second batch\n    \n    # Calculate the start and end index for the second batch\n    batch_start = batch_idx * batch_size\n    batch_end = min((batch_idx + 1) * batch_size, len(sample_submission))\n    batch_data = sample_submission.iloc[batch_start:batch_end]\n\n    print(f\"\\nProcessing batch {batch_idx + 1}/{num_batches}\")\n\n    try:\n        scorer = PerplexityCalculator(\n            model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n            load_in_8bit=False\n        )\n\n        for idx, row in batch_data.iterrows():\n            specific_solution = pd.DataFrame({'id': [row['id']], 'text': [row['text']]})\n            text_sequence = row['text'].split()\n\n            optimized_text, final_score, log_energies = ga.solve(row['text'], scorer)\n\n            print(f\"ID: {row['id']}, Final Perplexity: {final_score}\")\n            results.append({'id': row['id'], 'text': optimized_text})\n\n        temp_df = pd.DataFrame(results)\n        temp_df.to_csv(f\"submission_temp_batch_{batch_idx+1}.csv\", index=False)\n\n    except Exception as e:\n        print(f\"Error processing batch {batch_idx + 1}: {str(e)}\")\n        for idx, row in batch_data.iterrows():\n            results.append({'id': row['id'], 'text': row['text']})\n            print(\"results >>>>>>>>>>>>>>>>\")\n            print({'id': row['id'], 'text': row['text']})\n            print(\"------------------------------------------------\")\n\n    # Final submission for the second batch\n    submission = pd.DataFrame(results)\n    submission.to_csv(\"submission_id-5.csv\", index=False)\n    return submission\n\nif __name__ == \"__main__\":\n    print(\"Starting GA optimization...\")\n    final_submission = optimize_sequences_with_ga()\n    print(\"Optimization with GA completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:38:07.634187Z","iopub.execute_input":"2024-12-22T18:38:07.634545Z","iopub.status.idle":"2024-12-22T19:04:37.765177Z","shell.execute_reply.started":"2024-12-22T18:38:07.634514Z","shell.execute_reply":"2024-12-22T19:04:37.764368Z"}},"outputs":[{"name":"stdout","text":"Starting GA optimization...\n\nProcessing batch 2/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40386155ea6f4d3ca5f5e4c34e8b6872"}},"metadata":{}},{"name":"stdout","text":"Generation 1/20 - Best Perplexity: 786.1271512208434\nGeneration 2/20 - Best Perplexity: 699.2579777721731\nGeneration 3/20 - Best Perplexity: 608.8964823056507\nGeneration 4/20 - Best Perplexity: 571.1810975204792\nGeneration 5/20 - Best Perplexity: 529.6956665489097\nGeneration 6/20 - Best Perplexity: 520.519328864813\nGeneration 7/20 - Best Perplexity: 485.67502259252456\nGeneration 8/20 - Best Perplexity: 453.0076893306675\nGeneration 9/20 - Best Perplexity: 424.7089026586955\nGeneration 10/20 - Best Perplexity: 413.866201188035\nGeneration 11/20 - Best Perplexity: 404.86913074332483\nGeneration 12/20 - Best Perplexity: 402.5139595181153\nGeneration 13/20 - Best Perplexity: 399.896852844494\nGeneration 14/20 - Best Perplexity: 395.08999673949427\nGeneration 15/20 - Best Perplexity: 390.33161385620883\nGeneration 16/20 - Best Perplexity: 388.277753654066\nGeneration 17/20 - Best Perplexity: 382.49290715563285\nGeneration 18/20 - Best Perplexity: 380.9027081990725\nGeneration 19/20 - Best Perplexity: 347.0133760943923\nGeneration 20/20 - Best Perplexity: 345.32211686508504\n\nGenetic Algorithm Optimization Complete\nBest Perplexity: 344.9177782124224\nID: 5, Final Perplexity: 344.9177782124224\nOptimization with GA completed!\n","output_type":"stream"}],"execution_count":5}]}