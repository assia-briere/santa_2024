{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T18:36:39.930508Z",
     "iopub.status.busy": "2024-12-22T18:36:39.930212Z",
     "iopub.status.idle": "2024-12-22T18:36:40.255666Z",
     "shell.execute_reply": "2024-12-22T18:36:40.254714Z",
     "shell.execute_reply.started": "2024-12-22T18:36:39.930486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model.safetensors.index.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00001-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/config.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00003-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00002-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00007-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/README.md\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00008-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer_config.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00005-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00006-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/special_tokens_map.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/.gitattributes\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/tokenizer.model\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/model-00004-of-00008.safetensors\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/generation_config.json\n",
      "/kaggle/input/gemma-2/transformers/gemma-2-9b/2/transformers/transformers-4.42.0.dev0-py3-none-any.whl\n",
      "/kaggle/input/santa1/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:36:45.489625Z",
     "iopub.status.busy": "2024-12-22T18:36:45.489176Z",
     "iopub.status.idle": "2024-12-22T18:36:47.244094Z",
     "shell.execute_reply": "2024-12-22T18:36:47.243297Z",
     "shell.execute_reply.started": "2024-12-22T18:36:45.489595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: /kaggle/input/gemma-2/transformers/gemma-2-9b/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-9b\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:37:09.120837Z",
     "iopub.status.busy": "2024-12-22T18:37:09.120493Z",
     "iopub.status.idle": "2024-12-22T18:37:13.038311Z",
     "shell.execute_reply": "2024-12-22T18:37:13.037565Z",
     "shell.execute_reply.started": "2024-12-22T18:37:09.120812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    def __init__(self, model_path: str, load_in_8bit: bool = False, device_map: str = 'auto'):\n",
    "        # Charger le tokenizer depuis le modèle spécifié, avec option pour exécuter du code distant\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Si load_in_8bit est activé, configurer le modèle pour le chargement en 8 bits\n",
    "        if load_in_8bit:\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        else:\n",
    "            # Charger le modèle normalement avec le type de données approprié\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=device_map,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "        # Fonction de perte pour le calcul de la perplexité\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.model.eval()  # Mettre le modèle en mode évaluation\n",
    "\n",
    "    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 8) -> Union[float, List[float]]:\n",
    "        # Vérifier si l'entrée est une chaîne unique ou une liste\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "        loss_list = []\n",
    "\n",
    "        # Traiter les textes en lots pour le calcul de la perplexité\n",
    "        for i in range(0, len(input_texts), batch_size):\n",
    "            batch_texts = input_texts[i:i + batch_size]  # Créer un lot de textes\n",
    "            with torch.no_grad():  # Désactiver le calcul des gradients\n",
    "                model_inputs = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors='pt',  # Retourner des tenseurs PyTorch\n",
    "                    padding=True,  # Activer le padding pour les entrées de longueur variable\n",
    "                    truncation=True,  # Troncature des textes trop longs\n",
    "                    max_length=512,  # Longueur maximale des séquences\n",
    "                    add_special_tokens=True,  # Ajouter des tokens spéciaux si nécessaire\n",
    "                ).to(self.model.device)\n",
    "\n",
    "                # Obtenir les logits du modèle\n",
    "                output = self.model(**model_inputs)\n",
    "                logits = output.logits\n",
    "\n",
    "                # Préparer les labels pour le calcul de la perte\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n",
    "\n",
    "                # Calculer la perte\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculer la perte par batch\n",
    "                batch_losses = loss.view(len(batch_texts), -1).sum(dim=1) / model_inputs['attention_mask'].sum(dim=1)\n",
    "                loss_list.extend(batch_losses.cpu().tolist())  # Ajouter les pertes calculées à la liste\n",
    "\n",
    "        # Calculer la perplexité à partir des pertes\n",
    "        ppl = [np.exp(i) for i in loss_list]\n",
    "        return ppl[0] if single_input else ppl  # Retourner la perplexité pour une entrée unique ou en liste\n",
    "\n",
    "    def clear_memory(self) -> None:\n",
    "        # Libérer la mémoire en supprimant le modèle et le tokenizer\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        gc.collect()  # Forcer le ramassage des ordures pour libérer la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:37:30.895252Z",
     "iopub.status.busy": "2024-12-22T18:37:30.894753Z",
     "iopub.status.idle": "2024-12-22T18:37:30.908627Z",
     "shell.execute_reply": "2024-12-22T18:37:30.907698Z",
     "shell.execute_reply.started": "2024-12-22T18:37:30.895221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gc\n",
    "\n",
    "# Définir la classe GeneticAlgorithm\n",
    "class GeneticAlgorithm:\n",
    "    def __init__(self, random_state, population_size, generations, mutation_rate, elite_rate):\n",
    "        random.seed(random_state)  # Initialiser le générateur aléatoire avec un état donné\n",
    "        self.population_size = population_size  # Taille de la population\n",
    "        self.generations = generations  # Nombre de générations à parcourir\n",
    "        self.mutation_rate = mutation_rate  # Taux de mutation\n",
    "        self.elite_rate = elite_rate  # Taux d'élitisme\n",
    "\n",
    "    def order_crossover(self, parent1, parent2):\n",
    "        # Effectuer le croisement de type \"order\" entre deux parents\n",
    "        start, end = sorted(random.sample(range(len(parent1)), 2))  # Choisir deux indices aléatoires\n",
    "        child = [None] * len(parent1)  # Initialiser l'enfant avec des None\n",
    "        child[start:end+1] = parent1[start:end+1]  # Copier la section du premier parent\n",
    "        remaining = parent2.copy()  # Copier le deuxième parent\n",
    "        for used_word in child:\n",
    "            if used_word is not None:\n",
    "                remaining.remove(used_word)  # Retirer les mots utilisés de l'autre parent\n",
    "        j = 0\n",
    "        for i in range(len(child)):\n",
    "            if child[i] is None:\n",
    "                child[i] = remaining[j]  # Remplir l'enfant avec les mots restants\n",
    "                j += 1\n",
    "        return child\n",
    "\n",
    "    def mutate(self, individual):\n",
    "        # Appliquer une mutation à un individu\n",
    "        if random.random() < self.mutation_rate:  # Vérifier si la mutation doit avoir lieu\n",
    "            i, j = random.sample(range(len(individual)), 2)  # Sélectionner deux indices aléatoires\n",
    "            individual[i], individual[j] = individual[j], individual[i]  # Échanger les mots\n",
    "        return individual\n",
    "\n",
    "    def gradient_descent(self, sequence, scorer, learning_rate=0.01, max_iterations=20):\n",
    "        # Appliquer la descente de gradient pour améliorer une séquence\n",
    "        current_sequence = sequence[:]\n",
    "        best_sequence = sequence[:]\n",
    "        best_perplexity = scorer.get_perplexity(' '.join(best_sequence))  # Calculer la perplexité de la meilleure séquence\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            neighbor = current_sequence[:]  # Créer un voisin en copiant la séquence actuelle\n",
    "            i, j = random.sample(range(len(neighbor)), 2)  # Sélectionner deux indices aléatoires\n",
    "            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]  # Échanger les mots pour créer un voisin\n",
    "            neighbor_perplexity = scorer.get_perplexity(' '.join(neighbor))  # Calculer la perplexité du voisin\n",
    "            delta = neighbor_perplexity - best_perplexity  # Calculer le changement de perplexité\n",
    "            if delta < 0:  # Si le voisin est meilleur\n",
    "                current_sequence = neighbor[:]  # Mettre à jour la séquence actuelle\n",
    "                if neighbor_perplexity < best_perplexity:  # Si le voisin est meilleur que le meilleur\n",
    "                    best_sequence = neighbor[:]  # Mettre à jour la meilleure séquence\n",
    "                    best_perplexity = neighbor_perplexity  # Mettre à jour la meilleure perplexité\n",
    "\n",
    "        return best_sequence  # Retourner la meilleure séquence trouvée\n",
    "\n",
    "    def solve(self, text, scorer):\n",
    "        # Résoudre le problème en utilisant l'algorithme génétique\n",
    "        words = text.split()  # Diviser le texte en mots\n",
    "        word_N = len(words)  # Compter le nombre de mots\n",
    "        # Initialiser la population avec des permutations aléatoires des mots\n",
    "        population = [random.sample(words, word_N) for _ in range(self.population_size)]\n",
    "        log_energies = []  # Liste pour suivre les perplexités minimales de chaque génération\n",
    "\n",
    "        for generation in range(self.generations):\n",
    "            # Évaluer la fitness de chaque individu dans la population\n",
    "            fitness_scores = [scorer.get_perplexity(' '.join(ind)) for ind in population]\n",
    "            min_perplexity = min(fitness_scores)  # Trouver la perplexité minimale\n",
    "            log_energies.append(min_perplexity)  # Enregistrer la perplexité minimale\n",
    "            print(f\"Generation {generation + 1}/{self.generations} - Best Perplexity: {min_perplexity}\")\n",
    "\n",
    "            # Sélectionner les meilleurs individus (élites)\n",
    "            elite_indices = sorted(range(len(fitness_scores)),\n",
    "                                   key=lambda k: fitness_scores[k])[:int(self.population_size * self.elite_rate)]\n",
    "            new_population = [population[i] for i in elite_indices]  # Créer une nouvelle population avec les élites\n",
    "\n",
    "            # Compléter la nouvelle population jusqu'à la taille d'origine\n",
    "            while len(new_population) < self.population_size:\n",
    "                parent1, parent2 = random.sample(population, 2)  # Sélectionner deux parents aléatoires\n",
    "                child = self.order_crossover(parent1, parent2)  # Créer un enfant par croisement\n",
    "                new_population.append(child)  # Ajouter l'enfant à la nouvelle population\n",
    "\n",
    "            # Appliquer la mutation à tous les individus de la nouvelle population\n",
    "            for i in range(len(new_population)):\n",
    "                new_population[i] = self.mutate(new_population[i])\n",
    "\n",
    "            # Appliquer la descente de gradient aux élites\n",
    "            for i in range(int(len(new_population) * self.elite_rate)):\n",
    "                new_population[i] = self.gradient_descent(new_population[i], scorer)\n",
    "\n",
    "            population = new_population  # Mettre à jour la population\n",
    "\n",
    "        # Évaluer la fitness finale de la population\n",
    "        fitness_scores = [scorer.get_perplexity(' '.join(ind)) for ind in population]\n",
    "        best_individual = population[np.argmin(fitness_scores)]  # Trouver le meilleur individu\n",
    "        best_energy = min(fitness_scores)  # Meilleure perplexité\n",
    "        print(\"\\nGenetic Algorithm Optimization Complete\")\n",
    "        print(f\"Best Perplexity: {best_energy}\")\n",
    "\n",
    "        return ' '.join(best_individual), best_energy, log_energies  # Retourner le meilleur texte, sa perplexité et les enregistrements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-22T12:32:09.365Z",
     "iopub.execute_input": "2024-12-22T08:56:08.948869Z",
     "iopub.status.busy": "2024-12-22T08:56:08.948556Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GA optimization...\n",
      "\n",
      "Processing batch 1/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701a9363c40d4e4eadfa328f9bdcb44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/10 - Best Perplexity: 850.8240325202685\n",
      "Generation 2/10 - Best Perplexity: 621.4654194025813\n",
      "Generation 3/10 - Best Perplexity: 610.160500583438\n",
      "Generation 4/10 - Best Perplexity: 374.05994496126846\n",
      "Generation 5/10 - Best Perplexity: 374.05994496126846\n",
      "Generation 6/10 - Best Perplexity: 350.0905961076616\n",
      "Generation 7/10 - Best Perplexity: 336.633887513654\n",
      "Generation 8/10 - Best Perplexity: 336.633887513654\n",
      "Generation 9/10 - Best Perplexity: 317.99778895332923\n",
      "Generation 10/10 - Best Perplexity: 317.99778895332923\n",
      "\n",
      "Genetic Algorithm Optimization Complete\n",
      "Best Perplexity: 317.99778895332923\n",
      "ID: 0, Final Perplexity: 317.99778895332923\n",
      "Generation 1/10 - Best Perplexity: 3136.3788011047473\n",
      "Generation 2/10 - Best Perplexity: 2321.0732058825593\n",
      "Generation 3/10 - Best Perplexity: 2002.2406771787246\n",
      "Generation 4/10 - Best Perplexity: 1554.5083846251587\n",
      "Generation 5/10 - Best Perplexity: 919.6807368245869\n",
      "Generation 6/10 - Best Perplexity: 804.781899278158\n",
      "Generation 7/10 - Best Perplexity: 788.1449549394282\n",
      "Generation 8/10 - Best Perplexity: 737.1530972124707\n",
      "Generation 9/10 - Best Perplexity: 714.8979033568139\n",
      "Generation 10/10 - Best Perplexity: 580.5065812631666\n",
      "\n",
      "Genetic Algorithm Optimization Complete\n",
      "Best Perplexity: 541.2645272826893\n",
      "ID: 1, Final Perplexity: 541.2645272826893\n",
      "Generation 1/10 - Best Perplexity: 1546.5979744656236\n",
      "Generation 2/10 - Best Perplexity: 746.832842771766\n",
      "Generation 3/10 - Best Perplexity: 471.3765724312954\n",
      "Generation 4/10 - Best Perplexity: 471.3765724312954\n",
      "Generation 5/10 - Best Perplexity: 471.3765724312954\n",
      "Generation 6/10 - Best Perplexity: 471.3765724312954\n",
      "Generation 7/10 - Best Perplexity: 471.3765724312954\n",
      "Generation 8/10 - Best Perplexity: 471.3765724312954\n",
      "Generation 9/10 - Best Perplexity: 447.940117570023\n",
      "Generation 10/10 - Best Perplexity: 445.9782889278772\n",
      "\n",
      "Genetic Algorithm Optimization Complete\n",
      "Best Perplexity: 414.1109835593593\n",
      "ID: 2, Final Perplexity: 414.1109835593593\n",
      "Generation 1/10 - Best Perplexity: 1805.5688988357972\n",
      "Generation 2/10 - Best Perplexity: 1091.3383901267391\n",
      "Generation 3/10 - Best Perplexity: 856.8325363487495\n",
      "Generation 4/10 - Best Perplexity: 766.6444131853264\n",
      "Generation 5/10 - Best Perplexity: 766.04694808772\n",
      "Generation 6/10 - Best Perplexity: 721.3736253908652\n",
      "Generation 7/10 - Best Perplexity: 714.3823229676407\n",
      "Generation 8/10 - Best Perplexity: 683.856505210549\n",
      "Generation 9/10 - Best Perplexity: 681.3235893072133\n",
      "Generation 10/10 - Best Perplexity: 681.3235893072133\n",
      "\n",
      "Genetic Algorithm Optimization Complete\n",
      "Best Perplexity: 681.3235893072133\n",
      "ID: 3, Final Perplexity: 681.3235893072133\n",
      "Generation 1/10 - Best Perplexity: 1367.0466511406983\n",
      "Generation 2/10 - Best Perplexity: 888.9342976469443\n",
      "Generation 3/10 - Best Perplexity: 832.6462633737408\n",
      "Generation 4/10 - Best Perplexity: 797.0806668983332\n",
      "Generation 5/10 - Best Perplexity: 757.2572658364999\n",
      "Generation 6/10 - Best Perplexity: 680.7932661087748\n",
      "Generation 7/10 - Best Perplexity: 666.2180069923927\n",
      "Generation 8/10 - Best Perplexity: 648.385480850872\n",
      "Generation 9/10 - Best Perplexity: 622.5482910565527\n",
      "Generation 10/10 - Best Perplexity: 602.6192965550892\n",
      "\n",
      "Genetic Algorithm Optimization Complete\n",
      "Best Perplexity: 599.7740731472473\n",
      "ID: 4, Final Perplexity: 599.7740731472473\n",
      "\n",
      "Processing batch 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5303943e1342eaa6d7f997fc3af7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fonction pour optimiser des séquences avec l'algorithme génétique (GA)\n",
    "def optimize_sequences_with_ga(batch_size=5):\n",
    "    # Charger le fichier de soumission d'exemple\n",
    "    sample_submission = pd.read_csv(\"/kaggle/input/santa1/sample_submission.csv\")\n",
    "    results = []  # Liste pour stocker les résultats\n",
    "\n",
    "    # Initialiser l'algorithme génétique avec des paramètres spécifiques\n",
    "    ga = GeneticAlgorithm(\n",
    "        random_state=42,  # État aléatoire pour la reproductibilité\n",
    "        population_size=10,  # Taille de la population pour GA\n",
    "        generations=10,  # Nombre de générations d'optimisation\n",
    "        mutation_rate=0.1,  # Taux de mutation\n",
    "        elite_rate=0.2  # Taux d'élitisme\n",
    "    )\n",
    "\n",
    "    # Calculer le nombre de lots à traiter\n",
    "    num_batches = (len(sample_submission) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Traiter chaque lot de données\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size  # Index de début pour le lot\n",
    "        batch_end = min((batch_idx + 1) * batch_size, len(sample_submission))  # Index de fin pour le lot\n",
    "        batch_data = sample_submission.iloc[batch_start:batch_end]  # Extraire le lot de données\n",
    "\n",
    "        print(f\"\\nProcessing batch {batch_idx + 1}/{num_batches}\")  # Afficher le numéro de lot en cours de traitement\n",
    "\n",
    "        try:\n",
    "            # Initialiser le calculateur de perplexité avec le modèle spécifié\n",
    "            scorer = PerplexityCalculator(\n",
    "                model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "                load_in_8bit=False\n",
    "            )\n",
    "\n",
    "            # Traiter chaque ligne dans le lot\n",
    "            for idx, row in batch_data.iterrows():\n",
    "                specific_solution = pd.DataFrame({'id': [row['id']], 'text': [row['text']]})  # Créer un DataFrame pour la solution spécifique\n",
    "                text_sequence = row['text'].split()  # Diviser le texte en mots\n",
    "\n",
    "                # Optimiser le texte en utilisant l'algorithme génétique\n",
    "                optimized_text, final_score, log_energies = ga.solve(row['text'], scorer)\n",
    "\n",
    "                # Afficher le résultat final pour chaque ID\n",
    "                print(f\"ID: {row['id']}, Final Perplexity: {final_score}\")\n",
    "                results.append({'id': row['id'], 'text': optimized_text})  # Ajouter le résultat à la liste\n",
    "\n",
    "            # Enregistrer les résultats temporaires du lot dans un fichier CSV\n",
    "            temp_df = pd.DataFrame(results)\n",
    "            temp_df.to_csv(f\"submission_temp_batch_{batch_idx+1}.csv\", index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Gérer les erreurs durant le traitement des lots\n",
    "            print(f\"Error processing batch {batch_idx + 1}: {str(e)}\")\n",
    "            # Si une erreur se produit, ajouter les textes originaux aux résultats\n",
    "            for idx, row in batch_data.iterrows():\n",
    "                results.append({'id': row['id'], 'text': row['text']})\n",
    "                print(\"results >>>>>>>>>>>>>>>>\")\n",
    "                print({'id': row['id'], 'text': row['text']})  # Afficher le résultat original\n",
    "                print(\"------------------------------------------------\")\n",
    "\n",
    "    # Créer le DataFrame final avec tous les résultats et l'enregistrer dans un fichier CSV\n",
    "    submission = pd.DataFrame(results)\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    return submission  # Retourner le DataFrame de soumission final\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting GA optimization...\")\n",
    "    final_submission = optimize_sequences_with_ga()  # Lancer l'optimisation\n",
    "    print(\"Optimization with GA completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après l'optimisation de l'algorithme génétique, il est à noter que le batch 2, qui contient la séquence 6, s'est arrêté en raison de contraintes de ressources. Pour surmonter ce problème, nous avons décidé d'exécuter le runner de manière autonome dans la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T17:30:46.264444Z",
     "iopub.status.busy": "2024-12-22T17:30:46.264059Z",
     "iopub.status.idle": "2024-12-22T17:45:25.305646Z",
     "shell.execute_reply": "2024-12-22T17:45:25.304711Z",
     "shell.execute_reply.started": "2024-12-22T17:30:46.264416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GA optimization...\n",
      "\n",
      "Processing batch 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d6bfa0ec404461a8df15d667cc63ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/10 - Best Perplexity: 786.1271512208434\n",
      "Generation 2/10 - Best Perplexity: 699.2579777721731\n",
      "Generation 3/10 - Best Perplexity: 608.8964823056507\n",
      "Generation 4/10 - Best Perplexity: 571.1810975204792\n",
      "Generation 5/10 - Best Perplexity: 529.6956665489097\n",
      "Generation 6/10 - Best Perplexity: 520.519328864813\n",
      "Generation 7/10 - Best Perplexity: 485.67502259252456\n",
      "Generation 8/10 - Best Perplexity: 453.0076893306675\n",
      "Generation 9/10 - Best Perplexity: 424.7089026586955\n",
      "Generation 10/10 - Best Perplexity: 413.866201188035\n",
      "\n",
      "Genetic Algorithm Optimization Complete\n",
      "Best Perplexity: 404.86913074332483\n",
      "ID: 5, Final Perplexity: 404.86913074332483\n",
      "Optimization with GA completed!\n"
     ]
    }
   ],
   "source": [
    "# Function to optimize sequences with GA for second batch\n",
    "def optimize_sequences_with_ga(batch_size=5):\n",
    "    sample_submission = pd.read_csv(\"/kaggle/input/santa1/sample_submission.csv\")\n",
    "    results = []\n",
    "\n",
    "    ga = GeneticAlgorithm(\n",
    "        random_state=42,\n",
    "        population_size=10,\n",
    "        generations=10,\n",
    "        mutation_rate=0.1,\n",
    "        elite_rate=0.2\n",
    "    )\n",
    "\n",
    "    # Calculate the total number of batches\n",
    "    num_batches = (len(sample_submission) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Set batch_idx to 1 for the second batch (index starts at 0)\n",
    "    batch_idx = 1  # Only process the second batch\n",
    "    \n",
    "    # Calculate the start and end index for the second batch\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end = min((batch_idx + 1) * batch_size, len(sample_submission))\n",
    "    batch_data = sample_submission.iloc[batch_start:batch_end]\n",
    "\n",
    "    print(f\"\\nProcessing batch {batch_idx + 1}/{num_batches}\")\n",
    "\n",
    "    try:\n",
    "        scorer = PerplexityCalculator(\n",
    "            model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "            load_in_8bit=False\n",
    "        )\n",
    "\n",
    "        for idx, row in batch_data.iterrows():\n",
    "            specific_solution = pd.DataFrame({'id': [row['id']], 'text': [row['text']]})\n",
    "            text_sequence = row['text'].split()\n",
    "\n",
    "            optimized_text, final_score, log_energies = ga.solve(row['text'], scorer)\n",
    "\n",
    "            print(f\"ID: {row['id']}, Final Perplexity: {final_score}\")\n",
    "            results.append({'id': row['id'], 'text': optimized_text})\n",
    "\n",
    "        temp_df = pd.DataFrame(results)\n",
    "        temp_df.to_csv(f\"submission_temp_batch_{batch_idx+1}.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_idx + 1}: {str(e)}\")\n",
    "        for idx, row in batch_data.iterrows():\n",
    "            results.append({'id': row['id'], 'text': row['text']})\n",
    "            print(\"results >>>>>>>>>>>>>>>>\")\n",
    "            print({'id': row['id'], 'text': row['text']})\n",
    "            print(\"------------------------------------------------\")\n",
    "\n",
    "    # Final submission for the second batch\n",
    "    submission = pd.DataFrame(results)\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    return submission\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting GA optimization...\")\n",
    "    final_submission = optimize_sequences_with_ga()\n",
    "    print(\"Optimization with GA completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6332174,
     "sourceId": 10239648,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332270,
     "sourceId": 10239771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6342818,
     "sourceId": 10254000,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6350743,
     "sourceId": 10265362,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
